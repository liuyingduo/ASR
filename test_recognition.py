# test_recognition.py
from faster_whisper import WhisperModel
import pyaudio
import numpy as np
import time

# ä½¿ç”¨å·²ç»åŠ è½½çš„æ¨¡å‹é…ç½®
model = WhisperModel(
    "large-v2",
    device="cuda",
    compute_type="float16",
    download_root="./whisper_models"
)


def record_audio(duration=3):
    """å½•åˆ¶éŸ³é¢‘"""
    p = pyaudio.PyAudio()

    try:
        print(f"ğŸ¤ å½•åˆ¶ {duration} ç§’éŸ³é¢‘...")

        # éŸ³é¢‘å‚æ•°
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000

        # è‡ªåŠ¨é€‰æ‹©è¾“å…¥è®¾å¤‡
        input_device_index = None
        for i in range(p.get_device_count()):
            device_info = p.get_device_info_by_index(i)
            if device_info['maxInputChannels'] > 0:
                input_device_index = i
                print(f"ä½¿ç”¨è¾“å…¥è®¾å¤‡: {device_info['name']}")
                break

        stream = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            input_device_index=input_device_index,
            frames_per_buffer=CHUNK
        )

        frames = []
        total_chunks = int(RATE / CHUNK * duration)

        start_time = time.time()
        for i in range(total_chunks):
            data = stream.read(CHUNK, exception_on_overflow=False)
            frames.append(data)
            progress = (i + 1) / total_chunks
            bar = "â–ˆ" * int(30 * progress) + "â–‘" * (30 - int(30 * progress))
            print(f"\rå½•åˆ¶è¿›åº¦: [{bar}] {progress * 100:.1f}%", end="", flush=True)

        record_time = time.time() - start_time

        stream.stop_stream()
        stream.close()

        print(f"\nâœ… å½•åˆ¶å®Œæˆï¼Œè€—æ—¶: {record_time:.2f}ç§’")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
        audio_float = audio_data.astype(np.float32) / 32768.0

        return audio_float, record_time

    except Exception as e:
        print(f"âŒ å½•åˆ¶é”™è¯¯: {e}")
        return None, 0
    finally:
        p.terminate()


def transcribe_audio(audio_data):
    """è½¬å½•éŸ³é¢‘"""
    print("ğŸ”Š Large-v2æ¨¡å‹è¯†åˆ«ä¸­...")

    start_time = time.time()

    try:
        segments, info = model.transcribe(
            audio_data,
            language="zh",
            beam_size=5,
            best_of=3,
            temperature=0.0,
            vad_filter=True,
            vad_parameters=dict(
                min_silence_duration_ms=500,
                speech_pad_ms=200
            )
        )

        texts = []
        for segment in segments:
            texts.append(segment.text.strip())
            print(f"   - {segment.text}")

        text = " ".join(texts).strip()
        transcribe_time = time.time() - start_time

        return text, transcribe_time, info

    except Exception as e:
        print(f"âŒ è¯†åˆ«é”™è¯¯: {e}")
        return "", 0, None


# ä¸»ç¨‹åº
print("ğŸ‰ Large-v2æ¨¡å‹æµ‹è¯•")
print("=" * 50)

try:
    while True:
        # input("æŒ‰å›è½¦å¼€å§‹å½•éŸ³ï¼ˆ3ç§’ï¼‰...")

        # å½•åˆ¶éŸ³é¢‘
        audio_data, record_time = record_audio(3)
        if audio_data is None:
            continue

        # è½¬å½•éŸ³é¢‘
        text, transcribe_time, info = transcribe_audio(audio_data)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 40)
        if text and len(text) > 1:
            print(f"ğŸ—£ï¸  è¯†åˆ«ç»“æœ: {text}")
        else:
            print("âŒ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")

        total_time = record_time + transcribe_time
        print(f"â±ï¸  å½•åˆ¶: {record_time:.2f}s")
        print(f"â±ï¸  è¯†åˆ«: {transcribe_time:.2f}s")
        print(f"â±ï¸  æ€»å»¶è¿Ÿ: {total_time:.2f}s")
        print("=" * 40)

except KeyboardInterrupt:
    print("\nğŸ‘‹ æµ‹è¯•ç»“æŸ")
